{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ant_TD3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO2t9dA+HxFeKeJxEA2dGO4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/oguzhanbzglu/Reinforcement-Learning-DDPG/blob/main/ant_TD3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34HrD-1vQfMe"
      },
      "source": [
        "#**Twin-Delayed DDPG**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VD_sJZLOQt3W"
      },
      "source": [
        "##Installing the packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zfJPkw3OJgZC",
        "outputId": "8446141b-ca12-417a-d59f-0aa1ad653010"
      },
      "source": [
        "!pip install pybullet"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pybullet\n",
            "  Downloading pybullet-3.1.7.tar.gz (79.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 79.0 MB 59 kB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pybullet\n",
            "  Building wheel for pybullet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pybullet: filename=pybullet-3.1.7-cp37-cp37m-linux_x86_64.whl size=89751175 sha256=f3eab4351fc4836a0d068056109b0b2b25258f0291f5e650ea12bbaee7756142\n",
            "  Stored in directory: /root/.cache/pip/wheels/70/1c/62/86c8b68885c24123d87c5392d6678aa2b68a1796c8113e1aa6\n",
            "Successfully built pybullet\n",
            "Installing collected packages: pybullet\n",
            "Successfully installed pybullet-3.1.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8odOZE6Q1t1"
      },
      "source": [
        "## Importing the libraries "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rMxvxmEKdjh"
      },
      "source": [
        "import os\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pybullet_envs\n",
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from gym import wrappers\n",
        "from torch.autograd import Variable\n",
        "from collections import deque"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5usgFy8Q7J6"
      },
      "source": [
        "## **Step 1:** We initialize the Experience Replay memory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9rqBeJdMIrD"
      },
      "source": [
        "class ReplayBuffer(object):\n",
        "  \n",
        "  def __init__(self, max_size=1e6):\n",
        "    self.storage = [] #depoladığımız geçmiş\n",
        "    self.max_size = max_size #capacity\n",
        "    self.ptr = 0 #indexes of cells\n",
        "\n",
        "  def add(self, transition):\n",
        "    if len(self.storage) == self.max_size:\n",
        "      #burada kapasite dolunca; eskilerin yerine yeni değerleri ekleme yapıyoruz\n",
        "      self.storage[int(self.ptr)] = transition\n",
        "      self.ptr = (self.ptr + 1) % self.max_size\n",
        "    else:\n",
        "      self.storage.append(transition)\n",
        "      \n",
        "  def sample(self, batch_size):\n",
        "    ind = np.random.randint(0, len(self.storage), size = batch_size) #index\n",
        "    batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = [], [], [], [], []\n",
        "    for i in ind:\n",
        "      state, next_state, action, reward, done = self.storage[i]\n",
        "      batch_states.append(np.array(state, copy=False))\n",
        "      batch_next_states.append(np.array(next_state, copy=False))\n",
        "      batch_actions.append(np.array(action, copy=False))\n",
        "      batch_rewards.append(np.array(reward, copy=False))\n",
        "      batch_dones.append(np.array(done, copy=False))\n",
        "    return np.array(batch_states), np.array(batch_next_states), np.array(batch_actions), np.array(batch_rewards).reshape(-1,1), np.array(batch_dones).reshape(-1,1)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnBEhnJaVbys"
      },
      "source": [
        "## **Step 2:** We build one neural network for the Actor model and one build neural network for the Actor target"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVxBFoVmVa2A"
      },
      "source": [
        "# iki tane birbirşnşn aynısı NN inşa edeceğimiz için kolaylık olsun diye bunu sınıf(Class) oluşturarak yaparız\n",
        "class Actor(nn.Module): #PyTorch kütüphanesinden inheritance yaptık\n",
        "\n",
        "  def __init__(self, state_dim, action_dim, max_action):\n",
        "    super(Actor, self).__init__()\n",
        "    self.layer_1 = nn.Linear(state_dim, 400)\n",
        "    self.layer_2 = nn.Linear(400, 300)\n",
        "    self.layer_3 = nn.Linear(300, action_dim)\n",
        "    self.max_action = max_action\n",
        "  \n",
        "  #Here we implement the activation functions.\n",
        "  def forward(self, x): \n",
        "    x = F.relu(self.layer_1(x))\n",
        "    x = F.relu(self.layer_2(x))\n",
        "    x = self.max_action * torch.tanh(self.layer_3(x))\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzJeZue-bWBi"
      },
      "source": [
        "## **Step 3:** We build two neural networks for the Critic models and two build neural networks for the Critic targets "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxM0glAnbUBB"
      },
      "source": [
        "# 4 tane birbirinin aynısı NN inşa edeceğimiz için kolaylık olsun diye bunu sınıf(Class) oluşturarak yaparız\n",
        "class Critic(nn.Module): #PyTorch kütüphanesinden inheritance yaptık\n",
        "\n",
        "  def __init__(self, state_dim, action_dim):\n",
        "    super(Critic, self).__init__()\n",
        "    # Defining the first Critic neural network\n",
        "    self.layer_1 = nn.Linear(state_dim + action_dim, 400)\n",
        "    self.layer_2 = nn.Linear(400, 300)\n",
        "    self.layer_3 = nn.Linear(300, 1)\n",
        "    # Defining the second Critic neural network\n",
        "    self.layer_4 = nn.Linear(state_dim + action_dim, 400)\n",
        "    self.layer_5 = nn.Linear(400, 300)\n",
        "    self.layer_6 = nn.Linear(300, 1)\n",
        "  \n",
        "  # Here we implement the activation functions.\n",
        "  def forward(self, x, u): #x:states u:action \n",
        "    xu = torch.cat([x, u], 1) #inputs concaneted, axis=1\n",
        "    # Forward-Propagation on the first Critic neural network \n",
        "    x1 = F.relu(self.layer_1(xu))\n",
        "    x1 = F.relu(self.layer_2(x1))\n",
        "    x1 = self.layer_3(x1)\n",
        "    # Forward-Propagation on the second Critic neural network \n",
        "    x2 = F.relu(self.layer_4(xu))\n",
        "    x2 = F.relu(self.layer_5(x2))\n",
        "    x2 = self.layer_6(x2)\n",
        "    return x1, x2\n",
        "\n",
        "  # extra method\n",
        "  def Q1(self, x, u): #x:states u:action \n",
        "    xu = torch.cat([x, u], 1)  \n",
        "    x1 = F.relu(self.layer_1(xu))\n",
        "    x1 = F.relu(self.layer_2(x1))\n",
        "    x1 = self.layer_3(x1)\n",
        "    return x1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7x2q7ZUNlzyG"
      },
      "source": [
        "##**Step 4 to 15:** Training Process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZ16kKoflyoL"
      },
      "source": [
        "# Selecting the device (CPU or GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Building the whole Training Process into a class\n",
        "\n",
        "class TD3(object):\n",
        "\n",
        "  def __init__(self, state_dim, action_dim, max_action):\n",
        "    self.actor = Actor(state_dim, action_dim, max_action).to(device) # actor model\n",
        "    self.actor_target = Actor(state_dim, action_dim, max_action).to(device) # actor target\n",
        "    self.actor_target.load_state_dict(self.actor.state_dict()) # actor target for pre-trained\n",
        "    self.actor_optimizer = torch.optim.Adam(self.actor.parameters()) #actor optimize\n",
        "    self.critic = Critic(state_dim, action_dim).to(device) # critic model\n",
        "    self.critic_target = Critic(state_dim, action_dim).to(device) # critic target\n",
        "    self.critic_target.load_state_dict(self.critic.state_dict()) # critic target for pre-trained\n",
        "    self.critic_optimizer = torch.optim.Adam(self.critic.parameters()) # critic optimize\n",
        "    self.max_action = max_action\n",
        "    \n",
        "  def select_action(self, state):\n",
        "    state = torch.Tensor(state.reshape(1,-1)).to(device)\n",
        "    return self.actor(state).cpu().data.numpy().flatten()\n",
        "\n",
        "  def train(self, replay_buffer, iterations, batch_size=100, discount=0.99, tau = 0.005, policy_noise = 0.2, noise_clip = 0.5, policy_freq = 2):\n",
        "\n",
        "    for it in range(iterations):\n",
        "\n",
        "      #Step 4: We sample a batch of transitions (s, s’, a, r) from the memory\n",
        "      batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = replay_buffer.sample(batch_size) #step1, sample method \n",
        "      state = torch.Tensor(batch_states).to(device)\n",
        "      next_state = torch.Tensor(batch_next_states).to(device)\n",
        "      action = torch.Tensor(batch_actions).to(device)\n",
        "      reward = torch.Tensor(batch_rewards).to(device)\n",
        "      done = torch.Tensor(batch_dones).to(device)\n",
        "\n",
        "      #Step 5: From the next state s’, the Actor target plays the next action a’\n",
        "      next_action = self.actor_target(next_state)\n",
        "\n",
        "      #Step 6: We add Gaussian noise to this next action a' and we clamp it in a range of values supported by the environment\n",
        "      noise = torch.Tensor(batch_actions).data.normal_(0, policy_noise).to(device) #her eleman ve next action için gaussian noise yarattık\n",
        "      noise = noise.clamp(-noise_clip, noise_clip) #clip the Gaussian noise\n",
        "      next_action = (next_action + noise).clamp(-self.max_action, self.max_action) #add the Gaussian noise to our next actions\n",
        "\n",
        "      #Step 7: The two Critic targets take each the couple (s’, a’) as input and return two Q-values Qt1(s’,a’) and Qt2(s’,a’) as outputs\n",
        "      target_Q1, target_Q2 = self.critic_target(next_state, next_action)\n",
        "\n",
        "      #Step 8: We keep the minimum of those two Q-values: min(Qt1, Qt2)\n",
        "      target_Q = torch.min(target_Q1, target_Q2) \n",
        "\n",
        "      #Step 9: We get the final target of the two Critic models, which is: Qt = r + γ * min(Qt1, Qt2), where γ is the discount factor\n",
        "      target_Q = reward + ((1 - done) * discount * target_Q).detach()\n",
        "\n",
        "      #Step 10: The two Critic models take each the couple (s,a) as input and return two Q-values Q1(s,a) and Q2(s,a) as outputs\n",
        "      current_Q1, current_Q2 = self.critic(state, action)\n",
        "\n",
        "      #Step 11: We compute the loss coming from the two Critic models: Critic Loss = MSE_Loss(Q1(s,a), Qt) + MSE_Loss(Q2(s,a), Qt)\n",
        "      critic_loss = F.mse_loss(current_Q1,target_Q) + F.mse_loss(current_Q2,target_Q)\n",
        "\n",
        "      #Step 12: We backpropagate this Critic loss and update the parameters of the two Critic models with a SGD optimizer\n",
        "      self.critic_optimizer.zero_grad() #initialize the gradients optimizer\n",
        "      critic_loss.backward() #compute the gradient inside the new networks\n",
        "      self.critic_optimizer.step() #update the weight of the critic neural networks\n",
        "\n",
        "      #Step 13: Once every two iterations, we update our Actor model by performing gradient ascent on the output of the first Critic model.\n",
        "      if it % policy_freq == 0:\n",
        "        actor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n",
        "        self.actor_optimizer.zero_grad()\n",
        "        actor_loss.backward() \n",
        "        self.actor_optimizer.step()\n",
        "\n",
        "        #Step 14: Still once every two iterations, we update the weights of the Actor target by Polyak averaging.\n",
        "        for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
        "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data) #Polyak formulu\n",
        "\n",
        "        #Step 15: Still once every two iterations, we update the weights of the Critic target by Polyak averaging.\n",
        "        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
        "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data) #Polyak formulu\n",
        "  # Making a save method to save a trained model\n",
        "  def save(self, filename, directory):\n",
        "    torch.save(self.actor.state_dict(), '%s/%s_actor.pth' % (directory, filename))\n",
        "    torch.save(self.critic.state_dict(), '%s/%s_critic.pth' % (directory, filename))\n",
        "  \n",
        "  # Making a load method to load a pre-trained model\n",
        "  def load(self, filename, directory):\n",
        "    self.actor.load_state_dict(torch.load('%s/%s_actor.pth' % (directory, filename)))\n",
        "    self.critic.load_state_dict(torch.load('%s/%s_critic.pth' % (directory, filename)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWAn276cQSzw"
      },
      "source": [
        "## We make a function that evaluates the policy by calculating its average reward over 10 episodes "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0SNBZQT5dYG"
      },
      "source": [
        "def evaluate_policy(policy, eval_episodes=10):\n",
        "  avg_reward = 0.\n",
        "  for _ in range(eval_episodes):\n",
        "    obs = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "      action = policy.select_action(np.array(obs))\n",
        "      obs, reward, done, _ = env.step(action)\n",
        "      avg_reward += reward\n",
        "  avg_reward /= eval_episodes\n",
        "  print (\"---------------------------------------\")\n",
        "  print (\"Average Reward over the Evaluation Step: %f\" % (avg_reward))\n",
        "  print (\"---------------------------------------\")\n",
        "  return avg_reward"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76somxBFRLVm"
      },
      "source": [
        "## We set the parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQdjFGV-Qg5S"
      },
      "source": [
        "env_name = \"AntBulletEnv-v0\" # Name of a environment (set it to any Continous environment you want)\n",
        "seed = 0 # Random seed number\n",
        "start_timesteps = 1e4 # Number of iterations/timesteps before which the model randomly chooses an action, and after which it starts to use the policy network\n",
        "eval_freq = 5e3 # How often the evaluation step is performed (after how many timesteps)\n",
        "max_timesteps = 5e5 # Total number of iterations/timesteps\n",
        "save_models = True # Boolean checker whether or not to save the pre-trained model\n",
        "expl_noise = 0.1 # Exploration noise - STD value of exploration Gaussian noise\n",
        "batch_size = 100 # Size of the batch\n",
        "discount = 0.99 # Discount factor gamma, used in the calculation of the total discounted reward\n",
        "tau = 0.005 # Target network update rate\n",
        "policy_noise = 0.2 # STD of Gaussian noise added to the actions for the exploration purposes\n",
        "noise_clip = 0.5 # Maximum value of the Gaussian noise added to the actions (policy)\n",
        "policy_freq = 2 # Number of iterations to wait before the policy network (Actor model) is updated"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PON-Y6_-SH9U"
      },
      "source": [
        "## We create a file name for the two saved models: the Actor and Critic models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1erddrqqRRLh",
        "outputId": "74c0d732-9cc3-4107-ab73-c2a298e3611c"
      },
      "source": [
        "file_name = \"%s_%s_%s\" % (\"TD3\", env_name, str(seed))\n",
        "print (\"---------------------------------------\")\n",
        "print (\"Settings: %s\" % (file_name))\n",
        "print (\"---------------------------------------\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------------------------------\n",
            "Settings: TD3_AntBulletEnv-v0_0\n",
            "---------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwazcqY7SLB_"
      },
      "source": [
        "## We create a folder inside which will be saved the trained models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TsuSsb-dSQKI"
      },
      "source": [
        "if not os.path.exists(\"./results\"):\n",
        "  os.makedirs(\"./results\")\n",
        "if save_models and not os.path.exists(\"./pytorch_models\"):\n",
        "  os.makedirs(\"./pytorch_models\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_u-rwmVSTHl"
      },
      "source": [
        "## We create the PyBullet environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4fdEFnpSXTG",
        "outputId": "9d058c13-1908-4dc1-cffa-4fb28647c61f"
      },
      "source": [
        "env = gym.make(env_name)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHt6YvSxSZ82"
      },
      "source": [
        "## We set seeds and we get the necessary information on the states and actions in the chosen environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPe57c6FSZes"
      },
      "source": [
        "env.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.shape[0]\n",
        "max_action = float(env.action_space.high[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZ8jjIMMSfFB"
      },
      "source": [
        "## We create the policy network (the Actor model)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PIZ0EbQuSkCP"
      },
      "source": [
        "policy = TD3(state_dim, action_dim, max_action)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VGkqLKMNSnCa"
      },
      "source": [
        "## We create the Experience Replay memory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGF05yLvSo3T"
      },
      "source": [
        "replay_buffer = ReplayBuffer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfkiWZOpSrlk"
      },
      "source": [
        "## We define a list where all the evaluation results over 10 episodes are stored"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JEeuZ8ZLSuZC",
        "outputId": "94c68410-00a4-4cf5-fab1-bb6afadf9844"
      },
      "source": [
        "evaluations = [evaluate_policy(policy)]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 9.804960\n",
            "---------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJP34qtOSxVI"
      },
      "source": [
        "## We create a new folder directory in which the final results (videos of the agent) will be populated"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8CMn-VoSzy8"
      },
      "source": [
        "def mkdir(base, name):\n",
        "    path = os.path.join(base, name)\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "    return path\n",
        "work_dir = mkdir('exp', 'brs')\n",
        "monitor_dir = mkdir(work_dir, 'monitor')\n",
        "max_episode_steps = env._max_episode_steps\n",
        "save_env_vid = False\n",
        "if save_env_vid:\n",
        "  env = wrappers.Monitor(env, monitor_dir, force = True)\n",
        "  env.reset()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7wvBz7_VIwS"
      },
      "source": [
        "## We initialize the variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Y6Pb_ZiVMyj"
      },
      "source": [
        "total_timesteps = 0\n",
        "timesteps_since_eval = 0\n",
        "episode_num = 0\n",
        "done = True\n",
        "t0 = time.time()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkTlHalsVPdf"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "0T18hnfKVSkW",
        "outputId": "a0d80201-f3bc-4aac-df29-fa14a0442375"
      },
      "source": [
        "# We start the main loop over 500,000 timesteps\n",
        "while total_timesteps < max_timesteps:\n",
        "  \n",
        "  # If the episode is done\n",
        "  if done:\n",
        "\n",
        "    # If we are not at the very beginning, we start the training process of the model\n",
        "    if total_timesteps != 0:\n",
        "      print(\"Total Timesteps: {} Episode Num: {} Reward: {}\".format(total_timesteps, episode_num, episode_reward))\n",
        "      policy.train(replay_buffer, episode_timesteps, batch_size, discount, tau, policy_noise, noise_clip, policy_freq)\n",
        "\n",
        "    # We evaluate the episode and we save the policy\n",
        "    if timesteps_since_eval >= eval_freq:\n",
        "      timesteps_since_eval %= eval_freq\n",
        "      evaluations.append(evaluate_policy(policy))\n",
        "      policy.save(file_name, directory=\"./pytorch_models\")\n",
        "      np.save(\"./results/%s\" % (file_name), evaluations)\n",
        "    \n",
        "    # When the training step is done, we reset the state of the environment\n",
        "    obs = env.reset()\n",
        "    \n",
        "    # Set the Done to False\n",
        "    done = False\n",
        "    \n",
        "    # Set rewards and episode timesteps to zero\n",
        "    episode_reward = 0\n",
        "    episode_timesteps = 0\n",
        "    episode_num += 1\n",
        "  \n",
        "  # Before 10000 timesteps, we play random actions\n",
        "  if total_timesteps < start_timesteps:\n",
        "    action = env.action_space.sample()\n",
        "  else: # After 10000 timesteps, we switch to the model\n",
        "    action = policy.select_action(np.array(obs))\n",
        "    # If the explore_noise parameter is not 0, we add noise to the action and we clip it\n",
        "    if expl_noise != 0:\n",
        "      action = (action + np.random.normal(0, expl_noise, size=env.action_space.shape[0])).clip(env.action_space.low, env.action_space.high)\n",
        "  \n",
        "  # The agent performs the action in the environment, then reaches the next state and receives the reward\n",
        "  new_obs, reward, done, _ = env.step(action)\n",
        "  \n",
        "  # We check if the episode is done\n",
        "  done_bool = 0 if episode_timesteps + 1 == env._max_episode_steps else float(done)\n",
        "  \n",
        "  # We increase the total reward\n",
        "  episode_reward += reward\n",
        "  \n",
        "  # We store the new transition into the Experience Replay memory (ReplayBuffer)\n",
        "  replay_buffer.add((obs, new_obs, action, reward, done_bool))\n",
        "\n",
        "  # We update the state, the episode timestep, the total timesteps, and the timesteps since the evaluation of the policy\n",
        "  obs = new_obs\n",
        "  episode_timesteps += 1\n",
        "  total_timesteps += 1\n",
        "  timesteps_since_eval += 1\n",
        "\n",
        "# We add the last policy evaluation to our list of evaluations and we save our model\n",
        "evaluations.append(evaluate_policy(policy))\n",
        "if save_models: policy.save(\"%s\" % (file_name), directory=\"./pytorch_models\")\n",
        "np.save(\"./results/%s\" % (file_name), evaluations)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Timesteps: 1000 Episode Num: 1 Reward: 505.6437206706816\n",
            "Total Timesteps: 2000 Episode Num: 2 Reward: 502.39226299687164\n",
            "Total Timesteps: 3000 Episode Num: 3 Reward: 495.00229156960216\n",
            "Total Timesteps: 4000 Episode Num: 4 Reward: 446.28887883132245\n",
            "Total Timesteps: 5000 Episode Num: 5 Reward: 501.35212610868274\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 86.319340\n",
            "---------------------------------------\n",
            "Total Timesteps: 6000 Episode Num: 6 Reward: 492.7772672794931\n",
            "Total Timesteps: 6362 Episode Num: 7 Reward: 178.38054578188888\n",
            "Total Timesteps: 7362 Episode Num: 8 Reward: 500.097810746533\n",
            "Total Timesteps: 8362 Episode Num: 9 Reward: 522.0040946309833\n",
            "Total Timesteps: 9362 Episode Num: 10 Reward: 526.4280672540825\n",
            "Total Timesteps: 10362 Episode Num: 11 Reward: 501.3120575757632\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 86.797862\n",
            "---------------------------------------\n",
            "Total Timesteps: 11362 Episode Num: 12 Reward: 171.63825330215212\n",
            "Total Timesteps: 12362 Episode Num: 13 Reward: 115.96811386804318\n",
            "Total Timesteps: 13362 Episode Num: 14 Reward: 99.38264303767805\n",
            "Total Timesteps: 14362 Episode Num: 15 Reward: 96.3872879157604\n",
            "Total Timesteps: 15362 Episode Num: 16 Reward: 131.34951532924867\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 91.906718\n",
            "---------------------------------------\n",
            "Total Timesteps: 16362 Episode Num: 17 Reward: 92.386522058629\n",
            "Total Timesteps: 17362 Episode Num: 18 Reward: 90.08396254907422\n",
            "Total Timesteps: 18362 Episode Num: 19 Reward: 92.25809984864881\n",
            "Total Timesteps: 19362 Episode Num: 20 Reward: 88.23954670044468\n",
            "Total Timesteps: 20362 Episode Num: 21 Reward: 119.5797860493363\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 91.908878\n",
            "---------------------------------------\n",
            "Total Timesteps: 21362 Episode Num: 22 Reward: 109.16228775400596\n",
            "Total Timesteps: 22362 Episode Num: 23 Reward: 96.47242457815814\n",
            "Total Timesteps: 23362 Episode Num: 24 Reward: 95.38943810619307\n",
            "Total Timesteps: 24362 Episode Num: 25 Reward: 98.05365576376352\n",
            "Total Timesteps: 25362 Episode Num: 26 Reward: 132.85121825389638\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 94.641471\n",
            "---------------------------------------\n",
            "Total Timesteps: 26362 Episode Num: 27 Reward: 120.22323298696172\n",
            "Total Timesteps: 27362 Episode Num: 28 Reward: 115.18278317907294\n",
            "Total Timesteps: 28362 Episode Num: 29 Reward: 136.16004415232592\n",
            "Total Timesteps: 29362 Episode Num: 30 Reward: 108.45489067515668\n",
            "Total Timesteps: 30362 Episode Num: 31 Reward: 109.64391435715727\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 93.355745\n",
            "---------------------------------------\n",
            "Total Timesteps: 31362 Episode Num: 32 Reward: 107.78588837195058\n",
            "Total Timesteps: 32362 Episode Num: 33 Reward: 109.25109837125324\n",
            "Total Timesteps: 33362 Episode Num: 34 Reward: 101.87818004346927\n",
            "Total Timesteps: 34362 Episode Num: 35 Reward: 108.13339591504257\n",
            "Total Timesteps: 35362 Episode Num: 36 Reward: 117.61450891505605\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 96.025671\n",
            "---------------------------------------\n",
            "Total Timesteps: 36362 Episode Num: 37 Reward: 99.77971812707402\n",
            "Total Timesteps: 37362 Episode Num: 38 Reward: 116.7375465384683\n",
            "Total Timesteps: 38362 Episode Num: 39 Reward: 127.39955192286226\n",
            "Total Timesteps: 39362 Episode Num: 40 Reward: 97.36389626543276\n",
            "Total Timesteps: 40362 Episode Num: 41 Reward: 114.12788515985804\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 96.530070\n",
            "---------------------------------------\n",
            "Total Timesteps: 41362 Episode Num: 42 Reward: 100.35527718972541\n",
            "Total Timesteps: 42362 Episode Num: 43 Reward: 127.01746122006173\n",
            "Total Timesteps: 43362 Episode Num: 44 Reward: 115.9966010991167\n",
            "Total Timesteps: 44362 Episode Num: 45 Reward: 115.6968420461932\n",
            "Total Timesteps: 45362 Episode Num: 46 Reward: 115.41547084601922\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 94.084057\n",
            "---------------------------------------\n",
            "Total Timesteps: 46362 Episode Num: 47 Reward: 109.20276596618892\n",
            "Total Timesteps: 47362 Episode Num: 48 Reward: 105.67835337357695\n",
            "Total Timesteps: 48362 Episode Num: 49 Reward: 109.24088265319087\n",
            "Total Timesteps: 49362 Episode Num: 50 Reward: 108.77923902037061\n",
            "Total Timesteps: 50362 Episode Num: 51 Reward: 110.9209122830697\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 96.891646\n",
            "---------------------------------------\n",
            "Total Timesteps: 51362 Episode Num: 52 Reward: 115.05287306244085\n",
            "Total Timesteps: 52362 Episode Num: 53 Reward: 105.45548819454771\n",
            "Total Timesteps: 53362 Episode Num: 54 Reward: 100.15963099331111\n",
            "Total Timesteps: 54362 Episode Num: 55 Reward: 102.00630661085042\n",
            "Total Timesteps: 55362 Episode Num: 56 Reward: 108.99311305704197\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 97.635626\n",
            "---------------------------------------\n",
            "Total Timesteps: 56362 Episode Num: 57 Reward: 120.53391409853974\n",
            "Total Timesteps: 57362 Episode Num: 58 Reward: 106.90792475039301\n",
            "Total Timesteps: 58362 Episode Num: 59 Reward: 92.28216505825993\n",
            "Total Timesteps: 59362 Episode Num: 60 Reward: 104.42925149731832\n",
            "Total Timesteps: 60362 Episode Num: 61 Reward: 118.67303091983466\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 89.915882\n",
            "---------------------------------------\n",
            "Total Timesteps: 61362 Episode Num: 62 Reward: 117.35309309836148\n",
            "Total Timesteps: 62362 Episode Num: 63 Reward: 103.87494493226583\n",
            "Total Timesteps: 63362 Episode Num: 64 Reward: 188.73739946367226\n",
            "Total Timesteps: 64362 Episode Num: 65 Reward: 291.45353436500375\n",
            "Total Timesteps: 65362 Episode Num: 66 Reward: 212.43002707778356\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 99.811795\n",
            "---------------------------------------\n",
            "Total Timesteps: 66362 Episode Num: 67 Reward: 100.05140229162903\n",
            "Total Timesteps: 67362 Episode Num: 68 Reward: 216.30016547738674\n",
            "Total Timesteps: 68362 Episode Num: 69 Reward: 443.1705186056664\n",
            "Total Timesteps: 69362 Episode Num: 70 Reward: 277.3698599734081\n",
            "Total Timesteps: 70362 Episode Num: 71 Reward: 242.69080337598623\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 147.818998\n",
            "---------------------------------------\n",
            "Total Timesteps: 71362 Episode Num: 72 Reward: 236.98155372751916\n",
            "Total Timesteps: 72362 Episode Num: 73 Reward: 90.57633833803219\n",
            "Total Timesteps: 73362 Episode Num: 74 Reward: 69.46122967854103\n",
            "Total Timesteps: 74362 Episode Num: 75 Reward: 73.1736040391088\n",
            "Total Timesteps: 75362 Episode Num: 76 Reward: 73.21873064613315\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 49.141992\n",
            "---------------------------------------\n",
            "Total Timesteps: 76362 Episode Num: 77 Reward: 59.509754479606215\n",
            "Total Timesteps: 77362 Episode Num: 78 Reward: 75.47440670548059\n",
            "Total Timesteps: 78362 Episode Num: 79 Reward: 50.53108547036059\n",
            "Total Timesteps: 79362 Episode Num: 80 Reward: 131.2592340849006\n",
            "Total Timesteps: 79489 Episode Num: 81 Reward: 20.18464373388923\n",
            "Total Timesteps: 80489 Episode Num: 82 Reward: 216.65111827525487\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 103.350866\n",
            "---------------------------------------\n",
            "Total Timesteps: 80606 Episode Num: 83 Reward: 33.75477729327825\n",
            "Total Timesteps: 81606 Episode Num: 84 Reward: 185.74540172420043\n",
            "Total Timesteps: 81626 Episode Num: 85 Reward: -0.855061124177616\n",
            "Total Timesteps: 81646 Episode Num: 86 Reward: -0.6385187254610789\n",
            "Total Timesteps: 81666 Episode Num: 87 Reward: -0.7676043732801365\n",
            "Total Timesteps: 81686 Episode Num: 88 Reward: -0.6586272607774828\n",
            "Total Timesteps: 81706 Episode Num: 89 Reward: -0.8107539115870068\n",
            "Total Timesteps: 81726 Episode Num: 90 Reward: 0.05681819266580801\n",
            "Total Timesteps: 81746 Episode Num: 91 Reward: -0.6734914917599575\n",
            "Total Timesteps: 81766 Episode Num: 92 Reward: -0.6284492921279958\n",
            "Total Timesteps: 81786 Episode Num: 93 Reward: -0.31013371951740165\n",
            "Total Timesteps: 81806 Episode Num: 94 Reward: -0.10386395623200739\n",
            "Total Timesteps: 81829 Episode Num: 95 Reward: -0.35984070915110755\n",
            "Total Timesteps: 82829 Episode Num: 96 Reward: 229.653285660271\n",
            "Total Timesteps: 82856 Episode Num: 97 Reward: -1.2699564287366303\n",
            "Total Timesteps: 83084 Episode Num: 98 Reward: 37.608893007091126\n",
            "Total Timesteps: 84084 Episode Num: 99 Reward: 363.4888834555442\n",
            "Total Timesteps: 85084 Episode Num: 100 Reward: 294.0615545519511\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 330.861440\n",
            "---------------------------------------\n",
            "Total Timesteps: 86084 Episode Num: 101 Reward: 207.55530200995383\n",
            "Total Timesteps: 87084 Episode Num: 102 Reward: 428.3860807629029\n",
            "Total Timesteps: 88084 Episode Num: 103 Reward: 388.6991017333677\n",
            "Total Timesteps: 89084 Episode Num: 104 Reward: 282.2431071355866\n",
            "Total Timesteps: 90084 Episode Num: 105 Reward: 401.15101318371325\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 324.296905\n",
            "---------------------------------------\n",
            "Total Timesteps: 91084 Episode Num: 106 Reward: 144.38717597410553\n",
            "Total Timesteps: 91104 Episode Num: 107 Reward: 1.8476209561199055\n",
            "Total Timesteps: 91124 Episode Num: 108 Reward: 1.47097861165207\n",
            "Total Timesteps: 92124 Episode Num: 109 Reward: 202.55813298714526\n",
            "Total Timesteps: 93124 Episode Num: 110 Reward: 113.82105128079876\n",
            "Total Timesteps: 94017 Episode Num: 111 Reward: 137.39132740055817\n",
            "Total Timesteps: 95017 Episode Num: 112 Reward: 233.28099283253363\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 164.528218\n",
            "---------------------------------------\n",
            "Total Timesteps: 96017 Episode Num: 113 Reward: 200.7211329497325\n",
            "Total Timesteps: 97017 Episode Num: 114 Reward: 276.7930291566178\n",
            "Total Timesteps: 98017 Episode Num: 115 Reward: 372.9965717605181\n",
            "Total Timesteps: 99017 Episode Num: 116 Reward: 275.7794428554881\n",
            "Total Timesteps: 99039 Episode Num: 117 Reward: 1.2472816255662833\n",
            "Total Timesteps: 100039 Episode Num: 118 Reward: 410.56234278708183\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 366.349078\n",
            "---------------------------------------\n",
            "Total Timesteps: 101039 Episode Num: 119 Reward: 351.02963380111515\n",
            "Total Timesteps: 101059 Episode Num: 120 Reward: -3.0036127483708728\n",
            "Total Timesteps: 101079 Episode Num: 121 Reward: -3.4140820862456147\n",
            "Total Timesteps: 101099 Episode Num: 122 Reward: -2.9652457019914173\n",
            "Total Timesteps: 101119 Episode Num: 123 Reward: -3.237182056925949\n",
            "Total Timesteps: 101139 Episode Num: 124 Reward: -1.0412961006330796\n",
            "Total Timesteps: 101160 Episode Num: 125 Reward: -1.738799603056874\n",
            "Total Timesteps: 101181 Episode Num: 126 Reward: -2.4439041015862566\n",
            "Total Timesteps: 101201 Episode Num: 127 Reward: -1.8726926389992764\n",
            "Total Timesteps: 101221 Episode Num: 128 Reward: -1.3787773146431965\n",
            "Total Timesteps: 101242 Episode Num: 129 Reward: -3.474725092928069\n",
            "Total Timesteps: 101262 Episode Num: 130 Reward: -0.7538763890136084\n",
            "Total Timesteps: 101282 Episode Num: 131 Reward: -1.654044336553472\n",
            "Total Timesteps: 101302 Episode Num: 132 Reward: -0.3247635484584017\n",
            "Total Timesteps: 101323 Episode Num: 133 Reward: -2.3080975334575906\n",
            "Total Timesteps: 101345 Episode Num: 134 Reward: -4.159821400876427\n",
            "Total Timesteps: 101365 Episode Num: 135 Reward: -2.376335197445019\n",
            "Total Timesteps: 101385 Episode Num: 136 Reward: -0.5632652460342435\n",
            "Total Timesteps: 101405 Episode Num: 137 Reward: -1.1076711588531183\n",
            "Total Timesteps: 101425 Episode Num: 138 Reward: -1.8299611366741815\n",
            "Total Timesteps: 101445 Episode Num: 139 Reward: -1.9426506199794353\n",
            "Total Timesteps: 101467 Episode Num: 140 Reward: -3.2769959905991923\n",
            "Total Timesteps: 101487 Episode Num: 141 Reward: -2.1292656943721555\n",
            "Total Timesteps: 101507 Episode Num: 142 Reward: -1.0005180552330626\n",
            "Total Timesteps: 101527 Episode Num: 143 Reward: 0.026316614262207327\n",
            "Total Timesteps: 101547 Episode Num: 144 Reward: -1.303006217714991\n",
            "Total Timesteps: 101567 Episode Num: 145 Reward: -1.0295201029208485\n",
            "Total Timesteps: 101587 Episode Num: 146 Reward: -1.7532316135268309\n",
            "Total Timesteps: 101607 Episode Num: 147 Reward: -1.0698308876518547\n",
            "Total Timesteps: 101627 Episode Num: 148 Reward: -1.2894339594018347\n",
            "Total Timesteps: 101647 Episode Num: 149 Reward: -0.45999833272489354\n",
            "Total Timesteps: 101667 Episode Num: 150 Reward: -0.2936066660897023\n",
            "Total Timesteps: 101687 Episode Num: 151 Reward: -1.0049074902748352\n",
            "Total Timesteps: 101707 Episode Num: 152 Reward: -2.4747111513772375\n",
            "Total Timesteps: 101727 Episode Num: 153 Reward: -0.7053614567914646\n",
            "Total Timesteps: 101747 Episode Num: 154 Reward: -1.5856823231640629\n",
            "Total Timesteps: 101788 Episode Num: 155 Reward: 11.44845680900535\n",
            "Total Timesteps: 101860 Episode Num: 156 Reward: 25.514345227687535\n",
            "Total Timesteps: 101880 Episode Num: 157 Reward: 2.7246311260313414\n",
            "Total Timesteps: 101962 Episode Num: 158 Reward: 21.09478025954022\n",
            "Total Timesteps: 101983 Episode Num: 159 Reward: 1.2394953754703144\n",
            "Total Timesteps: 102003 Episode Num: 160 Reward: 3.2692245802216817\n",
            "Total Timesteps: 102023 Episode Num: 161 Reward: 0.20512275792601242\n",
            "Total Timesteps: 102043 Episode Num: 162 Reward: -0.37288649005110663\n",
            "Total Timesteps: 102092 Episode Num: 163 Reward: 7.810943221761018\n",
            "Total Timesteps: 103092 Episode Num: 164 Reward: 413.9189395946496\n",
            "Total Timesteps: 103112 Episode Num: 165 Reward: -0.2720606678856443\n",
            "Total Timesteps: 103132 Episode Num: 166 Reward: 1.4654126571128847\n",
            "Total Timesteps: 103152 Episode Num: 167 Reward: 1.4350099469428828\n",
            "Total Timesteps: 103172 Episode Num: 168 Reward: 2.0729416192983328\n",
            "Total Timesteps: 103192 Episode Num: 169 Reward: 4.077776892927913\n",
            "Total Timesteps: 103212 Episode Num: 170 Reward: 5.363917355584075\n",
            "Total Timesteps: 103232 Episode Num: 171 Reward: 6.543420885285794\n",
            "Total Timesteps: 103252 Episode Num: 172 Reward: 5.158331118425936\n",
            "Total Timesteps: 103272 Episode Num: 173 Reward: 4.08959218966626\n",
            "Total Timesteps: 103292 Episode Num: 174 Reward: 2.947217711705396\n",
            "Total Timesteps: 103312 Episode Num: 175 Reward: 3.0425567162775224\n",
            "Total Timesteps: 103332 Episode Num: 176 Reward: 6.6804813689525915\n",
            "Total Timesteps: 103352 Episode Num: 177 Reward: 6.602845793553115\n",
            "Total Timesteps: 103372 Episode Num: 178 Reward: 3.013895120802042\n",
            "Total Timesteps: 104372 Episode Num: 179 Reward: 431.75605859943886\n",
            "Total Timesteps: 104392 Episode Num: 180 Reward: 1.5540455151280916\n",
            "Total Timesteps: 104412 Episode Num: 181 Reward: 1.0665723355585248\n",
            "Total Timesteps: 104432 Episode Num: 182 Reward: 1.595016591472194\n",
            "Total Timesteps: 104452 Episode Num: 183 Reward: 1.0472845829681745\n",
            "Total Timesteps: 104472 Episode Num: 184 Reward: 1.0437783102103526\n",
            "Total Timesteps: 104492 Episode Num: 185 Reward: 1.5078242937529605\n",
            "Total Timesteps: 104512 Episode Num: 186 Reward: 2.004111992491812\n",
            "Total Timesteps: 104532 Episode Num: 187 Reward: 1.7391645908404954\n",
            "Total Timesteps: 104552 Episode Num: 188 Reward: 0.5176276165047393\n",
            "Total Timesteps: 104572 Episode Num: 189 Reward: 1.5480995316834107\n",
            "Total Timesteps: 104592 Episode Num: 190 Reward: 0.18729512117760505\n",
            "Total Timesteps: 105592 Episode Num: 191 Reward: 406.7218648984747\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 271.025361\n",
            "---------------------------------------\n",
            "Total Timesteps: 106592 Episode Num: 192 Reward: 317.9010229876074\n",
            "Total Timesteps: 107592 Episode Num: 193 Reward: 350.8493545427701\n",
            "Total Timesteps: 108592 Episode Num: 194 Reward: 591.9590389987012\n",
            "Total Timesteps: 109592 Episode Num: 195 Reward: 232.54537967841964\n",
            "Total Timesteps: 110592 Episode Num: 196 Reward: 312.84373288102\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 318.712095\n",
            "---------------------------------------\n",
            "Total Timesteps: 111592 Episode Num: 197 Reward: 268.29588982571704\n",
            "Total Timesteps: 112592 Episode Num: 198 Reward: 453.6725077468221\n",
            "Total Timesteps: 113592 Episode Num: 199 Reward: 522.2905323914747\n",
            "Total Timesteps: 114592 Episode Num: 200 Reward: 644.785103018682\n",
            "Total Timesteps: 115592 Episode Num: 201 Reward: 343.79844879253875\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 451.243425\n",
            "---------------------------------------\n",
            "Total Timesteps: 116592 Episode Num: 202 Reward: 435.51908704240884\n",
            "Total Timesteps: 117592 Episode Num: 203 Reward: 312.34195641685665\n",
            "Total Timesteps: 118592 Episode Num: 204 Reward: 510.61190552792095\n",
            "Total Timesteps: 119592 Episode Num: 205 Reward: 464.49011211524726\n",
            "Total Timesteps: 120592 Episode Num: 206 Reward: 462.0346606958205\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 392.047915\n",
            "---------------------------------------\n",
            "Total Timesteps: 121592 Episode Num: 207 Reward: 350.8125121104454\n",
            "Total Timesteps: 122592 Episode Num: 208 Reward: 502.7224610056586\n",
            "Total Timesteps: 123592 Episode Num: 209 Reward: 513.4551864022015\n",
            "Total Timesteps: 124592 Episode Num: 210 Reward: 429.8375779148366\n",
            "Total Timesteps: 125347 Episode Num: 211 Reward: 424.8015023448219\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 231.218211\n",
            "---------------------------------------\n",
            "Total Timesteps: 126347 Episode Num: 212 Reward: 222.59993347929506\n",
            "Total Timesteps: 127347 Episode Num: 213 Reward: 481.40284788242354\n",
            "Total Timesteps: 128347 Episode Num: 214 Reward: 422.07964908771237\n",
            "Total Timesteps: 128401 Episode Num: 215 Reward: 16.9087518134958\n",
            "Total Timesteps: 128516 Episode Num: 216 Reward: 26.88820255694483\n",
            "Total Timesteps: 128580 Episode Num: 217 Reward: 22.24614894825954\n",
            "Total Timesteps: 129580 Episode Num: 218 Reward: 362.0799607158965\n",
            "Total Timesteps: 130580 Episode Num: 219 Reward: 557.6466569624475\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 509.194649\n",
            "---------------------------------------\n",
            "Total Timesteps: 131580 Episode Num: 220 Reward: 616.894086028833\n",
            "Total Timesteps: 132580 Episode Num: 221 Reward: 484.0270804914322\n",
            "Total Timesteps: 133580 Episode Num: 222 Reward: 698.9422517966158\n",
            "Total Timesteps: 134580 Episode Num: 223 Reward: 299.546796563426\n",
            "Total Timesteps: 135580 Episode Num: 224 Reward: 451.18936984129283\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 528.821410\n",
            "---------------------------------------\n",
            "Total Timesteps: 136580 Episode Num: 225 Reward: 640.6499882384315\n",
            "Total Timesteps: 137580 Episode Num: 226 Reward: 782.1166141075153\n",
            "Total Timesteps: 138580 Episode Num: 227 Reward: 402.80075663514566\n",
            "Total Timesteps: 139580 Episode Num: 228 Reward: 655.2025195693686\n",
            "Total Timesteps: 140580 Episode Num: 229 Reward: 537.8412422059753\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 593.727795\n",
            "---------------------------------------\n",
            "Total Timesteps: 141580 Episode Num: 230 Reward: 540.0687806079329\n",
            "Total Timesteps: 141893 Episode Num: 231 Reward: 151.81852242369862\n",
            "Total Timesteps: 142893 Episode Num: 232 Reward: 406.86570482007625\n",
            "Total Timesteps: 143893 Episode Num: 233 Reward: 104.09790044660032\n",
            "Total Timesteps: 144893 Episode Num: 234 Reward: 560.1687054354027\n",
            "Total Timesteps: 145893 Episode Num: 235 Reward: 443.9763478053783\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 374.847411\n",
            "---------------------------------------\n",
            "Total Timesteps: 146893 Episode Num: 236 Reward: 476.0121463661661\n",
            "Total Timesteps: 147893 Episode Num: 237 Reward: 476.64399417928723\n",
            "Total Timesteps: 148893 Episode Num: 238 Reward: 465.7186021121956\n",
            "Total Timesteps: 149893 Episode Num: 239 Reward: 479.76048131291503\n",
            "Total Timesteps: 150893 Episode Num: 240 Reward: 406.6486788051857\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 494.569698\n",
            "---------------------------------------\n",
            "Total Timesteps: 151893 Episode Num: 241 Reward: 410.4300510574315\n",
            "Total Timesteps: 152893 Episode Num: 242 Reward: 393.14671292656425\n",
            "Total Timesteps: 153893 Episode Num: 243 Reward: 585.6758017395605\n",
            "Total Timesteps: 154893 Episode Num: 244 Reward: 619.478574325536\n",
            "Total Timesteps: 155893 Episode Num: 245 Reward: 491.0933179314088\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 588.111698\n",
            "---------------------------------------\n",
            "Total Timesteps: 156893 Episode Num: 246 Reward: 609.8633176426491\n",
            "Total Timesteps: 157893 Episode Num: 247 Reward: 550.0323411197954\n",
            "Total Timesteps: 158893 Episode Num: 248 Reward: 627.7140487124313\n",
            "Total Timesteps: 159893 Episode Num: 249 Reward: 539.099401135121\n",
            "Total Timesteps: 160893 Episode Num: 250 Reward: 588.9148527689191\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 521.574576\n",
            "---------------------------------------\n",
            "Total Timesteps: 161893 Episode Num: 251 Reward: 474.88768316969293\n",
            "Total Timesteps: 162893 Episode Num: 252 Reward: 783.2086880685004\n",
            "Total Timesteps: 163893 Episode Num: 253 Reward: 651.0366707123047\n",
            "Total Timesteps: 164893 Episode Num: 254 Reward: 429.7186532823343\n",
            "Total Timesteps: 165893 Episode Num: 255 Reward: 467.11546422069375\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 443.189220\n",
            "---------------------------------------\n",
            "Total Timesteps: 166893 Episode Num: 256 Reward: 593.4719315542778\n",
            "Total Timesteps: 167893 Episode Num: 257 Reward: 626.8202303131773\n",
            "Total Timesteps: 168893 Episode Num: 258 Reward: 523.0541440756868\n",
            "Total Timesteps: 169893 Episode Num: 259 Reward: 574.4099548020272\n",
            "Total Timesteps: 170893 Episode Num: 260 Reward: 547.5600976901923\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 495.218134\n",
            "---------------------------------------\n",
            "Total Timesteps: 171893 Episode Num: 261 Reward: 429.84032514146924\n",
            "Total Timesteps: 172893 Episode Num: 262 Reward: 581.1313893556376\n",
            "Total Timesteps: 173893 Episode Num: 263 Reward: 367.1017273909896\n",
            "Total Timesteps: 174893 Episode Num: 264 Reward: 501.0873042936415\n",
            "Total Timesteps: 175893 Episode Num: 265 Reward: 531.3351786882504\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 619.650817\n",
            "---------------------------------------\n",
            "Total Timesteps: 176893 Episode Num: 266 Reward: 735.140632974054\n",
            "Total Timesteps: 177893 Episode Num: 267 Reward: 536.8907885787316\n",
            "Total Timesteps: 178893 Episode Num: 268 Reward: 479.4816889655831\n",
            "Total Timesteps: 179893 Episode Num: 269 Reward: 809.8618285659838\n",
            "Total Timesteps: 180893 Episode Num: 270 Reward: 538.007275044551\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 742.525896\n",
            "---------------------------------------\n",
            "Total Timesteps: 181893 Episode Num: 271 Reward: 707.7406594848323\n",
            "Total Timesteps: 182893 Episode Num: 272 Reward: 726.2962248937147\n",
            "Total Timesteps: 183893 Episode Num: 273 Reward: 576.8467943564832\n",
            "Total Timesteps: 183923 Episode Num: 274 Reward: 10.077583373544053\n",
            "Total Timesteps: 184923 Episode Num: 275 Reward: 739.2914698106068\n",
            "Total Timesteps: 185599 Episode Num: 276 Reward: 423.2376263361304\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 377.975377\n",
            "---------------------------------------\n",
            "Total Timesteps: 186599 Episode Num: 277 Reward: 767.1207183858743\n",
            "Total Timesteps: 187599 Episode Num: 278 Reward: 740.204627146143\n",
            "Total Timesteps: 188599 Episode Num: 279 Reward: 509.2104551092231\n",
            "Total Timesteps: 189599 Episode Num: 280 Reward: 848.9188689408496\n",
            "Total Timesteps: 190599 Episode Num: 281 Reward: 756.7071413886658\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 502.072647\n",
            "---------------------------------------\n",
            "Total Timesteps: 191599 Episode Num: 282 Reward: 479.55283494890773\n",
            "Total Timesteps: 192599 Episode Num: 283 Reward: 276.62964742176524\n",
            "Total Timesteps: 193599 Episode Num: 284 Reward: 443.143042394601\n",
            "Total Timesteps: 194599 Episode Num: 285 Reward: 552.2678973909052\n",
            "Total Timesteps: 195599 Episode Num: 286 Reward: 778.5753361229151\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 554.024310\n",
            "---------------------------------------\n",
            "Total Timesteps: 196599 Episode Num: 287 Reward: 634.8363103924343\n",
            "Total Timesteps: 197599 Episode Num: 288 Reward: 580.908093524161\n",
            "Total Timesteps: 198599 Episode Num: 289 Reward: 664.9987186932054\n",
            "Total Timesteps: 199599 Episode Num: 290 Reward: 540.7958032912771\n",
            "Total Timesteps: 200599 Episode Num: 291 Reward: 484.0325981368159\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 418.507021\n",
            "---------------------------------------\n",
            "Total Timesteps: 201599 Episode Num: 292 Reward: 541.0240153769922\n",
            "Total Timesteps: 202599 Episode Num: 293 Reward: 441.2128113975963\n",
            "Total Timesteps: 203599 Episode Num: 294 Reward: 550.2518882858119\n",
            "Total Timesteps: 204599 Episode Num: 295 Reward: 448.3045121135921\n",
            "Total Timesteps: 205599 Episode Num: 296 Reward: 578.5384625917671\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 291.903919\n",
            "---------------------------------------\n",
            "Total Timesteps: 206599 Episode Num: 297 Reward: 264.86107690346677\n",
            "Total Timesteps: 207599 Episode Num: 298 Reward: 790.1491107950376\n",
            "Total Timesteps: 208599 Episode Num: 299 Reward: 244.04972560387048\n",
            "Total Timesteps: 209599 Episode Num: 300 Reward: 615.5720899779699\n",
            "Total Timesteps: 210599 Episode Num: 301 Reward: 818.2784380242692\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 623.407466\n",
            "---------------------------------------\n",
            "Total Timesteps: 211599 Episode Num: 302 Reward: 750.7837324646264\n",
            "Total Timesteps: 212599 Episode Num: 303 Reward: 645.521699673615\n",
            "Total Timesteps: 213599 Episode Num: 304 Reward: 642.0278229819884\n",
            "Total Timesteps: 214599 Episode Num: 305 Reward: 647.5740536531655\n",
            "Total Timesteps: 215599 Episode Num: 306 Reward: 580.9424583833733\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 604.284216\n",
            "---------------------------------------\n",
            "Total Timesteps: 216599 Episode Num: 307 Reward: 749.401237630121\n",
            "Total Timesteps: 217599 Episode Num: 308 Reward: 725.5422788518623\n",
            "Total Timesteps: 218599 Episode Num: 309 Reward: 743.2037845440238\n",
            "Total Timesteps: 219599 Episode Num: 310 Reward: 705.0930163554233\n",
            "Total Timesteps: 220599 Episode Num: 311 Reward: 561.3859071624436\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 651.467900\n",
            "---------------------------------------\n",
            "Total Timesteps: 221599 Episode Num: 312 Reward: 706.2004828173356\n",
            "Total Timesteps: 222599 Episode Num: 313 Reward: 442.88474509548314\n",
            "Total Timesteps: 223599 Episode Num: 314 Reward: 502.82830824462195\n",
            "Total Timesteps: 224599 Episode Num: 315 Reward: 794.4101967303324\n",
            "Total Timesteps: 225599 Episode Num: 316 Reward: 606.9481797770396\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 687.029709\n",
            "---------------------------------------\n",
            "Total Timesteps: 226599 Episode Num: 317 Reward: 670.0202211338615\n",
            "Total Timesteps: 227599 Episode Num: 318 Reward: 804.3265883281974\n",
            "Total Timesteps: 228599 Episode Num: 319 Reward: 588.8022408079606\n",
            "Total Timesteps: 229599 Episode Num: 320 Reward: 751.4089513391955\n",
            "Total Timesteps: 230599 Episode Num: 321 Reward: 543.7239900579299\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 578.706458\n",
            "---------------------------------------\n",
            "Total Timesteps: 231599 Episode Num: 322 Reward: 531.2477642018417\n",
            "Total Timesteps: 232599 Episode Num: 323 Reward: 413.58852560455585\n",
            "Total Timesteps: 233599 Episode Num: 324 Reward: 443.8921339965491\n",
            "Total Timesteps: 234599 Episode Num: 325 Reward: 569.0770247663127\n",
            "Total Timesteps: 235599 Episode Num: 326 Reward: 585.6791494438692\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 630.856009\n",
            "---------------------------------------\n",
            "Total Timesteps: 236599 Episode Num: 327 Reward: 770.4431878738496\n",
            "Total Timesteps: 237599 Episode Num: 328 Reward: 660.3308504101209\n",
            "Total Timesteps: 238599 Episode Num: 329 Reward: 784.0713347206193\n",
            "Total Timesteps: 239599 Episode Num: 330 Reward: 900.1320001150119\n",
            "Total Timesteps: 240599 Episode Num: 331 Reward: 804.2127178500944\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 548.817782\n",
            "---------------------------------------\n",
            "Total Timesteps: 241599 Episode Num: 332 Reward: 716.9580721754717\n",
            "Total Timesteps: 242599 Episode Num: 333 Reward: 666.9387225821282\n",
            "Total Timesteps: 243599 Episode Num: 334 Reward: 445.97082940582146\n",
            "Total Timesteps: 244599 Episode Num: 335 Reward: 596.2198092934923\n",
            "Total Timesteps: 245599 Episode Num: 336 Reward: 646.7617838528887\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 522.413579\n",
            "---------------------------------------\n",
            "Total Timesteps: 246599 Episode Num: 337 Reward: 636.2732002399292\n",
            "Total Timesteps: 247599 Episode Num: 338 Reward: 497.7732856098883\n",
            "Total Timesteps: 248599 Episode Num: 339 Reward: 866.2348553286295\n",
            "Total Timesteps: 249599 Episode Num: 340 Reward: 627.663986592816\n",
            "Total Timesteps: 250599 Episode Num: 341 Reward: 358.15626978031855\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 653.105444\n",
            "---------------------------------------\n",
            "Total Timesteps: 251599 Episode Num: 342 Reward: 626.7585526939525\n",
            "Total Timesteps: 252599 Episode Num: 343 Reward: 665.9882745660753\n",
            "Total Timesteps: 253599 Episode Num: 344 Reward: 711.783999183909\n",
            "Total Timesteps: 254599 Episode Num: 345 Reward: 579.3690870578829\n",
            "Total Timesteps: 255599 Episode Num: 346 Reward: 656.7735514065404\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 672.886839\n",
            "---------------------------------------\n",
            "Total Timesteps: 256599 Episode Num: 347 Reward: 681.705629594552\n",
            "Total Timesteps: 257599 Episode Num: 348 Reward: 711.2295020133636\n",
            "Total Timesteps: 258599 Episode Num: 349 Reward: 581.6176350365681\n",
            "Total Timesteps: 259599 Episode Num: 350 Reward: 651.3312729825153\n",
            "Total Timesteps: 260599 Episode Num: 351 Reward: 788.7956210126447\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 603.322818\n",
            "---------------------------------------\n",
            "Total Timesteps: 261599 Episode Num: 352 Reward: 701.4530509452826\n",
            "Total Timesteps: 262599 Episode Num: 353 Reward: 684.8527957184963\n",
            "Total Timesteps: 263599 Episode Num: 354 Reward: 611.2615068339102\n",
            "Total Timesteps: 264599 Episode Num: 355 Reward: 755.3810729822425\n",
            "Total Timesteps: 265599 Episode Num: 356 Reward: 719.5576773084714\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 527.626411\n",
            "---------------------------------------\n",
            "Total Timesteps: 266599 Episode Num: 357 Reward: 816.8454648655265\n",
            "Total Timesteps: 267599 Episode Num: 358 Reward: 641.2299272875283\n",
            "Total Timesteps: 268599 Episode Num: 359 Reward: 485.9100210292617\n",
            "Total Timesteps: 269599 Episode Num: 360 Reward: 540.731467470567\n",
            "Total Timesteps: 270599 Episode Num: 361 Reward: 559.6274299113174\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 557.500994\n",
            "---------------------------------------\n",
            "Total Timesteps: 271599 Episode Num: 362 Reward: 719.3798778096273\n",
            "Total Timesteps: 272599 Episode Num: 363 Reward: 855.1345035204506\n",
            "Total Timesteps: 273599 Episode Num: 364 Reward: 654.4793650697728\n",
            "Total Timesteps: 274599 Episode Num: 365 Reward: 419.8895102747146\n",
            "Total Timesteps: 275599 Episode Num: 366 Reward: 491.9569271351196\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 588.272372\n",
            "---------------------------------------\n",
            "Total Timesteps: 275907 Episode Num: 367 Reward: 260.6296854904989\n",
            "Total Timesteps: 276907 Episode Num: 368 Reward: 455.37687329544923\n",
            "Total Timesteps: 277907 Episode Num: 369 Reward: 719.958239887514\n",
            "Total Timesteps: 278907 Episode Num: 370 Reward: 862.141171941358\n",
            "Total Timesteps: 279907 Episode Num: 371 Reward: 337.7517012768851\n",
            "Total Timesteps: 280907 Episode Num: 372 Reward: 584.3200153891283\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 629.852651\n",
            "---------------------------------------\n",
            "Total Timesteps: 281907 Episode Num: 373 Reward: 562.2727980479098\n",
            "Total Timesteps: 282907 Episode Num: 374 Reward: 694.4994483708607\n",
            "Total Timesteps: 283907 Episode Num: 375 Reward: 403.14233143075614\n",
            "Total Timesteps: 284907 Episode Num: 376 Reward: 462.8801096898663\n",
            "Total Timesteps: 285907 Episode Num: 377 Reward: 684.5291436267663\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 693.358159\n",
            "---------------------------------------\n",
            "Total Timesteps: 286907 Episode Num: 378 Reward: 707.6953347062358\n",
            "Total Timesteps: 287907 Episode Num: 379 Reward: 453.6557233613472\n",
            "Total Timesteps: 288907 Episode Num: 380 Reward: 381.5016451821281\n",
            "Total Timesteps: 289907 Episode Num: 381 Reward: 744.494919036098\n",
            "Total Timesteps: 290907 Episode Num: 382 Reward: 744.0061419365268\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 696.446241\n",
            "---------------------------------------\n",
            "Total Timesteps: 291907 Episode Num: 383 Reward: 822.6729625277386\n",
            "Total Timesteps: 292907 Episode Num: 384 Reward: 585.8466313184292\n",
            "Total Timesteps: 293907 Episode Num: 385 Reward: 742.8921073900918\n",
            "Total Timesteps: 294907 Episode Num: 386 Reward: 660.5251103937321\n",
            "Total Timesteps: 295907 Episode Num: 387 Reward: 805.7017586722483\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 717.144771\n",
            "---------------------------------------\n",
            "Total Timesteps: 296907 Episode Num: 388 Reward: 740.6361939150095\n",
            "Total Timesteps: 297907 Episode Num: 389 Reward: 773.811836345353\n",
            "Total Timesteps: 298907 Episode Num: 390 Reward: 642.3776458887219\n",
            "Total Timesteps: 299907 Episode Num: 391 Reward: 807.6497878895232\n",
            "Total Timesteps: 300907 Episode Num: 392 Reward: 777.1514671270571\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 681.068366\n",
            "---------------------------------------\n",
            "Total Timesteps: 301907 Episode Num: 393 Reward: 787.3827995203301\n",
            "Total Timesteps: 302907 Episode Num: 394 Reward: 626.6385087338509\n",
            "Total Timesteps: 303907 Episode Num: 395 Reward: 790.7205080335406\n",
            "Total Timesteps: 304907 Episode Num: 396 Reward: 746.232739257001\n",
            "Total Timesteps: 305907 Episode Num: 397 Reward: 649.0107146410352\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 633.993672\n",
            "---------------------------------------\n",
            "Total Timesteps: 306907 Episode Num: 398 Reward: 813.0228293747035\n",
            "Total Timesteps: 307907 Episode Num: 399 Reward: 603.1055441079997\n",
            "Total Timesteps: 308907 Episode Num: 400 Reward: 641.1613465027497\n",
            "Total Timesteps: 309907 Episode Num: 401 Reward: 786.7236845001208\n",
            "Total Timesteps: 310907 Episode Num: 402 Reward: 474.5865176965759\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 761.087999\n",
            "---------------------------------------\n",
            "Total Timesteps: 311907 Episode Num: 403 Reward: 644.7443926348643\n",
            "Total Timesteps: 312907 Episode Num: 404 Reward: 859.9664121366333\n",
            "Total Timesteps: 313907 Episode Num: 405 Reward: 928.2023428083081\n",
            "Total Timesteps: 314907 Episode Num: 406 Reward: 923.6915441594887\n",
            "Total Timesteps: 315907 Episode Num: 407 Reward: 875.5952884920202\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 759.970587\n",
            "---------------------------------------\n",
            "Total Timesteps: 316907 Episode Num: 408 Reward: 625.4776858916733\n",
            "Total Timesteps: 317907 Episode Num: 409 Reward: 774.0424950499704\n",
            "Total Timesteps: 318907 Episode Num: 410 Reward: 781.9106150016563\n",
            "Total Timesteps: 319907 Episode Num: 411 Reward: 852.9588463263862\n",
            "Total Timesteps: 320907 Episode Num: 412 Reward: 868.5094569529072\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 595.918020\n",
            "---------------------------------------\n",
            "Total Timesteps: 321907 Episode Num: 413 Reward: 739.1946584280066\n",
            "Total Timesteps: 322907 Episode Num: 414 Reward: 942.0500372217816\n",
            "Total Timesteps: 323907 Episode Num: 415 Reward: 874.3501184759314\n",
            "Total Timesteps: 324907 Episode Num: 416 Reward: 574.7696113938738\n",
            "Total Timesteps: 325907 Episode Num: 417 Reward: 449.8497704132128\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 722.986376\n",
            "---------------------------------------\n",
            "Total Timesteps: 326907 Episode Num: 418 Reward: 591.93201892325\n",
            "Total Timesteps: 327907 Episode Num: 419 Reward: 403.1532442223833\n",
            "Total Timesteps: 328907 Episode Num: 420 Reward: 870.0496205307477\n",
            "Total Timesteps: 329907 Episode Num: 421 Reward: 712.294928404938\n",
            "Total Timesteps: 330907 Episode Num: 422 Reward: 858.2773250876689\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 779.109348\n",
            "---------------------------------------\n",
            "Total Timesteps: 331907 Episode Num: 423 Reward: 840.2777139952415\n",
            "Total Timesteps: 332907 Episode Num: 424 Reward: 729.5109649194782\n",
            "Total Timesteps: 333907 Episode Num: 425 Reward: 776.7034942352499\n",
            "Total Timesteps: 334907 Episode Num: 426 Reward: 759.9411396798472\n",
            "Total Timesteps: 335907 Episode Num: 427 Reward: 733.1728143077526\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 790.076912\n",
            "---------------------------------------\n",
            "Total Timesteps: 336907 Episode Num: 428 Reward: 952.9707652943661\n",
            "Total Timesteps: 337907 Episode Num: 429 Reward: 834.3302879228543\n",
            "Total Timesteps: 338907 Episode Num: 430 Reward: 878.8140851490674\n",
            "Total Timesteps: 339907 Episode Num: 431 Reward: 787.6465072222111\n",
            "Total Timesteps: 340907 Episode Num: 432 Reward: 642.2635855125384\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 822.310733\n",
            "---------------------------------------\n",
            "Total Timesteps: 341907 Episode Num: 433 Reward: 800.7415584003198\n",
            "Total Timesteps: 342907 Episode Num: 434 Reward: 938.4244806649386\n",
            "Total Timesteps: 343907 Episode Num: 435 Reward: 855.5926047552271\n",
            "Total Timesteps: 344907 Episode Num: 436 Reward: 555.2055744915856\n",
            "Total Timesteps: 345907 Episode Num: 437 Reward: 597.4119364173868\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 690.101794\n",
            "---------------------------------------\n",
            "Total Timesteps: 346907 Episode Num: 438 Reward: 533.4803733861669\n",
            "Total Timesteps: 347907 Episode Num: 439 Reward: 556.8559498453508\n",
            "Total Timesteps: 348907 Episode Num: 440 Reward: 889.1833440472361\n",
            "Total Timesteps: 349907 Episode Num: 441 Reward: 753.2926341856219\n",
            "Total Timesteps: 350907 Episode Num: 442 Reward: 722.4355668090504\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 798.546348\n",
            "---------------------------------------\n",
            "Total Timesteps: 351907 Episode Num: 443 Reward: 648.8030085554507\n",
            "Total Timesteps: 352907 Episode Num: 444 Reward: 896.4227747214711\n",
            "Total Timesteps: 353907 Episode Num: 445 Reward: 947.2527057220337\n",
            "Total Timesteps: 354907 Episode Num: 446 Reward: 1032.6756606579609\n",
            "Total Timesteps: 355907 Episode Num: 447 Reward: 649.2971645513758\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 830.593974\n",
            "---------------------------------------\n",
            "Total Timesteps: 356907 Episode Num: 448 Reward: 796.0562468623622\n",
            "Total Timesteps: 357907 Episode Num: 449 Reward: 865.6311671650049\n",
            "Total Timesteps: 358907 Episode Num: 450 Reward: 541.1284151471375\n",
            "Total Timesteps: 359907 Episode Num: 451 Reward: 655.21947814871\n",
            "Total Timesteps: 360907 Episode Num: 452 Reward: 1084.6634643144528\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 904.741733\n",
            "---------------------------------------\n",
            "Total Timesteps: 361907 Episode Num: 453 Reward: 802.4436376065618\n",
            "Total Timesteps: 362907 Episode Num: 454 Reward: 1105.2855315294096\n",
            "Total Timesteps: 363907 Episode Num: 455 Reward: 811.9111402346177\n",
            "Total Timesteps: 364907 Episode Num: 456 Reward: 836.4208613032988\n",
            "Total Timesteps: 365907 Episode Num: 457 Reward: 696.7376429216389\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 772.957594\n",
            "---------------------------------------\n",
            "Total Timesteps: 366907 Episode Num: 458 Reward: 687.5731603043816\n",
            "Total Timesteps: 367907 Episode Num: 459 Reward: 770.9456498956043\n",
            "Total Timesteps: 368907 Episode Num: 460 Reward: 1036.214695974053\n",
            "Total Timesteps: 369907 Episode Num: 461 Reward: 828.3379273099229\n",
            "Total Timesteps: 370907 Episode Num: 462 Reward: 1153.2509367134319\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 874.743966\n",
            "---------------------------------------\n",
            "Total Timesteps: 371907 Episode Num: 463 Reward: 799.0630117216754\n",
            "Total Timesteps: 372907 Episode Num: 464 Reward: 756.1542053474922\n",
            "Total Timesteps: 373907 Episode Num: 465 Reward: 1035.620432312052\n",
            "Total Timesteps: 374907 Episode Num: 466 Reward: 833.1703355783412\n",
            "Total Timesteps: 375907 Episode Num: 467 Reward: 711.583893890603\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 912.810013\n",
            "---------------------------------------\n",
            "Total Timesteps: 376907 Episode Num: 468 Reward: 927.0072736994655\n",
            "Total Timesteps: 377907 Episode Num: 469 Reward: 936.1755979036832\n",
            "Total Timesteps: 378907 Episode Num: 470 Reward: 967.5397050046884\n",
            "Total Timesteps: 379907 Episode Num: 471 Reward: 1156.4338217421193\n",
            "Total Timesteps: 380907 Episode Num: 472 Reward: 992.976400048969\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 946.563619\n",
            "---------------------------------------\n",
            "Total Timesteps: 381907 Episode Num: 473 Reward: 551.3560751587825\n",
            "Total Timesteps: 382907 Episode Num: 474 Reward: 578.1422718804599\n",
            "Total Timesteps: 383907 Episode Num: 475 Reward: 776.5242893866989\n",
            "Total Timesteps: 384907 Episode Num: 476 Reward: 851.1147218965979\n",
            "Total Timesteps: 385907 Episode Num: 477 Reward: 888.2852978967496\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1190.347206\n",
            "---------------------------------------\n",
            "Total Timesteps: 386907 Episode Num: 478 Reward: 1009.1272460964882\n",
            "Total Timesteps: 387907 Episode Num: 479 Reward: 1182.022084476674\n",
            "Total Timesteps: 388907 Episode Num: 480 Reward: 1329.4690547355378\n",
            "Total Timesteps: 389907 Episode Num: 481 Reward: 885.0017963229446\n",
            "Total Timesteps: 390907 Episode Num: 482 Reward: 1331.1087031648115\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1034.539870\n",
            "---------------------------------------\n",
            "Total Timesteps: 391907 Episode Num: 483 Reward: 605.4552808646016\n",
            "Total Timesteps: 392907 Episode Num: 484 Reward: 896.3595936517268\n",
            "Total Timesteps: 393907 Episode Num: 485 Reward: 964.459349644395\n",
            "Total Timesteps: 394907 Episode Num: 486 Reward: 1211.9518089997837\n",
            "Total Timesteps: 395907 Episode Num: 487 Reward: 772.053812437509\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1125.784098\n",
            "---------------------------------------\n",
            "Total Timesteps: 396907 Episode Num: 488 Reward: 1171.1430229070181\n",
            "Total Timesteps: 397907 Episode Num: 489 Reward: 1019.9312975096119\n",
            "Total Timesteps: 398907 Episode Num: 490 Reward: 803.6639123157415\n",
            "Total Timesteps: 399907 Episode Num: 491 Reward: 775.1663122839303\n",
            "Total Timesteps: 400907 Episode Num: 492 Reward: 849.8077040755221\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1136.088859\n",
            "---------------------------------------\n",
            "Total Timesteps: 401907 Episode Num: 493 Reward: 947.937300400229\n",
            "Total Timesteps: 402907 Episode Num: 494 Reward: 1188.9294649358976\n",
            "Total Timesteps: 403907 Episode Num: 495 Reward: 1327.514609768822\n",
            "Total Timesteps: 404907 Episode Num: 496 Reward: 868.5113464274542\n",
            "Total Timesteps: 405907 Episode Num: 497 Reward: 776.8860611074518\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1094.136832\n",
            "---------------------------------------\n",
            "Total Timesteps: 406907 Episode Num: 498 Reward: 992.2062851870119\n",
            "Total Timesteps: 407907 Episode Num: 499 Reward: 774.3958234509096\n",
            "Total Timesteps: 408907 Episode Num: 500 Reward: 1273.6404307151647\n",
            "Total Timesteps: 409907 Episode Num: 501 Reward: 462.131100971194\n",
            "Total Timesteps: 410907 Episode Num: 502 Reward: 1263.366142137028\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1189.901197\n",
            "---------------------------------------\n",
            "Total Timesteps: 411907 Episode Num: 503 Reward: 1102.112787491614\n",
            "Total Timesteps: 412907 Episode Num: 504 Reward: 713.7331272203852\n",
            "Total Timesteps: 413907 Episode Num: 505 Reward: 908.8943884512199\n",
            "Total Timesteps: 414907 Episode Num: 506 Reward: 1293.8441465076849\n",
            "Total Timesteps: 415907 Episode Num: 507 Reward: 920.2319535753629\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 931.880437\n",
            "---------------------------------------\n",
            "Total Timesteps: 416907 Episode Num: 508 Reward: 649.988623916311\n",
            "Total Timesteps: 417907 Episode Num: 509 Reward: 1051.66129865277\n",
            "Total Timesteps: 418907 Episode Num: 510 Reward: 680.7315657129465\n",
            "Total Timesteps: 419907 Episode Num: 511 Reward: 1276.6509303577322\n",
            "Total Timesteps: 420907 Episode Num: 512 Reward: 889.0604548487437\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1297.071747\n",
            "---------------------------------------\n",
            "Total Timesteps: 421907 Episode Num: 513 Reward: 1300.1257931126966\n",
            "Total Timesteps: 422907 Episode Num: 514 Reward: 1426.3326578992612\n",
            "Total Timesteps: 423907 Episode Num: 515 Reward: 814.9972172030625\n",
            "Total Timesteps: 424907 Episode Num: 516 Reward: 1231.215642010941\n",
            "Total Timesteps: 425907 Episode Num: 517 Reward: 966.2415736989997\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1281.570831\n",
            "---------------------------------------\n",
            "Total Timesteps: 426907 Episode Num: 518 Reward: 1373.2509771242753\n",
            "Total Timesteps: 427907 Episode Num: 519 Reward: 1393.2839717945042\n",
            "Total Timesteps: 428907 Episode Num: 520 Reward: 1060.533100749075\n",
            "Total Timesteps: 429907 Episode Num: 521 Reward: 1406.3675342468698\n",
            "Total Timesteps: 430907 Episode Num: 522 Reward: 1314.359224209837\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1383.237316\n",
            "---------------------------------------\n",
            "Total Timesteps: 431907 Episode Num: 523 Reward: 1372.0776532051348\n",
            "Total Timesteps: 432907 Episode Num: 524 Reward: 1371.0189191774248\n",
            "Total Timesteps: 433907 Episode Num: 525 Reward: 1238.3290618961232\n",
            "Total Timesteps: 434907 Episode Num: 526 Reward: 1428.2483545563914\n",
            "Total Timesteps: 435907 Episode Num: 527 Reward: 1304.7827743152557\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1293.785106\n",
            "---------------------------------------\n",
            "Total Timesteps: 436907 Episode Num: 528 Reward: 1309.674441965504\n",
            "Total Timesteps: 437907 Episode Num: 529 Reward: 1350.6371452947762\n",
            "Total Timesteps: 438907 Episode Num: 530 Reward: 1234.226838653219\n",
            "Total Timesteps: 439907 Episode Num: 531 Reward: 1038.1461524586784\n",
            "Total Timesteps: 440907 Episode Num: 532 Reward: 1386.3542390205728\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1194.079594\n",
            "---------------------------------------\n",
            "Total Timesteps: 441907 Episode Num: 533 Reward: 1270.4468074094425\n",
            "Total Timesteps: 442907 Episode Num: 534 Reward: 1345.7264998775825\n",
            "Total Timesteps: 443907 Episode Num: 535 Reward: 986.76876857149\n",
            "Total Timesteps: 444907 Episode Num: 536 Reward: 1357.1496014884308\n",
            "Total Timesteps: 445907 Episode Num: 537 Reward: 1340.0202970916505\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1437.843931\n",
            "---------------------------------------\n",
            "Total Timesteps: 446907 Episode Num: 538 Reward: 1451.4235851146643\n",
            "Total Timesteps: 447907 Episode Num: 539 Reward: 1318.4663913737074\n",
            "Total Timesteps: 448907 Episode Num: 540 Reward: 1379.2023172691502\n",
            "Total Timesteps: 449907 Episode Num: 541 Reward: 1389.599515401565\n",
            "Total Timesteps: 450907 Episode Num: 542 Reward: 1327.495369942534\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1431.333962\n",
            "---------------------------------------\n",
            "Total Timesteps: 451907 Episode Num: 543 Reward: 1408.8940424963516\n",
            "Total Timesteps: 452907 Episode Num: 544 Reward: 1554.4832733097667\n",
            "Total Timesteps: 453907 Episode Num: 545 Reward: 1566.2483959145964\n",
            "Total Timesteps: 454907 Episode Num: 546 Reward: 1378.149116789043\n",
            "Total Timesteps: 455907 Episode Num: 547 Reward: 1653.5246931647282\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1480.798840\n",
            "---------------------------------------\n",
            "Total Timesteps: 456907 Episode Num: 548 Reward: 1464.4968506338023\n",
            "Total Timesteps: 457907 Episode Num: 549 Reward: 1537.9403174150882\n",
            "Total Timesteps: 458907 Episode Num: 550 Reward: 1260.9251053565572\n",
            "Total Timesteps: 459907 Episode Num: 551 Reward: 1481.244837943895\n",
            "Total Timesteps: 460907 Episode Num: 552 Reward: 1501.7394660261873\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1499.171482\n",
            "---------------------------------------\n",
            "Total Timesteps: 461907 Episode Num: 553 Reward: 1526.535560702389\n",
            "Total Timesteps: 462907 Episode Num: 554 Reward: 1324.6282671037964\n",
            "Total Timesteps: 463907 Episode Num: 555 Reward: 1434.6055314925331\n",
            "Total Timesteps: 464907 Episode Num: 556 Reward: 1544.2937556214563\n",
            "Total Timesteps: 465907 Episode Num: 557 Reward: 1472.9508903262526\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1570.715423\n",
            "---------------------------------------\n",
            "Total Timesteps: 466907 Episode Num: 558 Reward: 1576.2274491445353\n",
            "Total Timesteps: 467907 Episode Num: 559 Reward: 1561.4821342880675\n",
            "Total Timesteps: 468907 Episode Num: 560 Reward: 1430.1566239272133\n",
            "Total Timesteps: 469907 Episode Num: 561 Reward: 1637.8150175423384\n",
            "Total Timesteps: 470907 Episode Num: 562 Reward: 1795.6840716549498\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1678.553683\n",
            "---------------------------------------\n",
            "Total Timesteps: 471907 Episode Num: 563 Reward: 1649.2025552841094\n",
            "Total Timesteps: 472907 Episode Num: 564 Reward: 1589.6608635947493\n",
            "Total Timesteps: 473907 Episode Num: 565 Reward: 1660.8162397575627\n",
            "Total Timesteps: 474907 Episode Num: 566 Reward: 1583.9038607759849\n",
            "Total Timesteps: 475907 Episode Num: 567 Reward: 1588.5528395834592\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1744.114285\n",
            "---------------------------------------\n",
            "Total Timesteps: 476907 Episode Num: 568 Reward: 1723.4701191834652\n",
            "Total Timesteps: 477907 Episode Num: 569 Reward: 1458.657357460121\n",
            "Total Timesteps: 478907 Episode Num: 570 Reward: 1677.5400293593375\n",
            "Total Timesteps: 479907 Episode Num: 571 Reward: 1806.5001164064972\n",
            "Total Timesteps: 480907 Episode Num: 572 Reward: 1788.2162391381141\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1777.341858\n",
            "---------------------------------------\n",
            "Total Timesteps: 481907 Episode Num: 573 Reward: 1776.1834693409883\n",
            "Total Timesteps: 482907 Episode Num: 574 Reward: 1796.0754458188896\n",
            "Total Timesteps: 483907 Episode Num: 575 Reward: 1887.8154503104793\n",
            "Total Timesteps: 484907 Episode Num: 576 Reward: 1748.3209264917039\n",
            "Total Timesteps: 485907 Episode Num: 577 Reward: 1517.4039125441802\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1832.224200\n",
            "---------------------------------------\n",
            "Total Timesteps: 486907 Episode Num: 578 Reward: 1810.3441650509844\n",
            "Total Timesteps: 487907 Episode Num: 579 Reward: 1769.9511158849955\n",
            "Total Timesteps: 488907 Episode Num: 580 Reward: 1898.1010754469823\n",
            "Total Timesteps: 489907 Episode Num: 581 Reward: 1857.4929461280155\n",
            "Total Timesteps: 490907 Episode Num: 582 Reward: 1878.5986244002015\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1789.076618\n",
            "---------------------------------------\n",
            "Total Timesteps: 491907 Episode Num: 583 Reward: 1791.7962333190717\n",
            "Total Timesteps: 492907 Episode Num: 584 Reward: 1803.1724581364786\n",
            "Total Timesteps: 493907 Episode Num: 585 Reward: 1763.7460789514328\n",
            "Total Timesteps: 494907 Episode Num: 586 Reward: 1830.3490840631434\n",
            "Total Timesteps: 495907 Episode Num: 587 Reward: 1660.6256111495118\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1863.447218\n",
            "---------------------------------------\n",
            "Total Timesteps: 496907 Episode Num: 588 Reward: 1872.264956142691\n",
            "Total Timesteps: 497907 Episode Num: 589 Reward: 1929.6884832674932\n",
            "Total Timesteps: 498907 Episode Num: 590 Reward: 1730.3891646417121\n",
            "Total Timesteps: 499907 Episode Num: 591 Reward: 1907.1455975406093\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1823.378577\n",
            "---------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJecwqlcYqLP"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JaEox3nDYr20",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87860db4-52e4-47ee-9be0-11c6ab538330"
      },
      "source": [
        "class Actor(nn.Module):\n",
        "  \n",
        "  def __init__(self, state_dim, action_dim, max_action):\n",
        "    super(Actor, self).__init__()\n",
        "    self.layer_1 = nn.Linear(state_dim, 400)\n",
        "    self.layer_2 = nn.Linear(400, 300)\n",
        "    self.layer_3 = nn.Linear(300, action_dim)\n",
        "    self.max_action = max_action\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.layer_1(x))\n",
        "    x = F.relu(self.layer_2(x))\n",
        "    x = self.max_action * torch.tanh(self.layer_3(x)) \n",
        "    return x\n",
        "\n",
        "class Critic(nn.Module):\n",
        "  \n",
        "  def __init__(self, state_dim, action_dim):\n",
        "    super(Critic, self).__init__()\n",
        "    # Defining the first Critic neural network\n",
        "    self.layer_1 = nn.Linear(state_dim + action_dim, 400)\n",
        "    self.layer_2 = nn.Linear(400, 300)\n",
        "    self.layer_3 = nn.Linear(300, 1)\n",
        "    # Defining the second Critic neural network\n",
        "    self.layer_4 = nn.Linear(state_dim + action_dim, 400)\n",
        "    self.layer_5 = nn.Linear(400, 300)\n",
        "    self.layer_6 = nn.Linear(300, 1)\n",
        "\n",
        "  def forward(self, x, u):\n",
        "    xu = torch.cat([x, u], 1)\n",
        "    # Forward-Propagation on the first Critic Neural Network\n",
        "    x1 = F.relu(self.layer_1(xu))\n",
        "    x1 = F.relu(self.layer_2(x1))\n",
        "    x1 = self.layer_3(x1)\n",
        "    # Forward-Propagation on the second Critic Neural Network\n",
        "    x2 = F.relu(self.layer_4(xu))\n",
        "    x2 = F.relu(self.layer_5(x2))\n",
        "    x2 = self.layer_6(x2)\n",
        "    return x1, x2\n",
        "\n",
        "  def Q1(self, x, u):\n",
        "    xu = torch.cat([x, u], 1)\n",
        "    x1 = F.relu(self.layer_1(xu))\n",
        "    x1 = F.relu(self.layer_2(x1))\n",
        "    x1 = self.layer_3(x1)\n",
        "    return x1\n",
        "\n",
        "# Selecting the device (CPU or GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Building the whole Training Process into a class\n",
        "\n",
        "class TD3(object):\n",
        "  \n",
        "  def __init__(self, state_dim, action_dim, max_action):\n",
        "    self.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
        "    self.actor_target = Actor(state_dim, action_dim, max_action).to(device)\n",
        "    self.actor_target.load_state_dict(self.actor.state_dict())\n",
        "    self.actor_optimizer = torch.optim.Adam(self.actor.parameters())\n",
        "    self.critic = Critic(state_dim, action_dim).to(device)\n",
        "    self.critic_target = Critic(state_dim, action_dim).to(device)\n",
        "    self.critic_target.load_state_dict(self.critic.state_dict())\n",
        "    self.critic_optimizer = torch.optim.Adam(self.critic.parameters())\n",
        "    self.max_action = max_action\n",
        "\n",
        "  def select_action(self, state):\n",
        "    state = torch.Tensor(state.reshape(1, -1)).to(device)\n",
        "    return self.actor(state).cpu().data.numpy().flatten()\n",
        "\n",
        "  def train(self, replay_buffer, iterations, batch_size=100, discount=0.99, tau=0.005, policy_noise=0.2, noise_clip=0.5, policy_freq=2):\n",
        "    \n",
        "    for it in range(iterations):\n",
        "      \n",
        "      # Step 4: We sample a batch of transitions (s, s’, a, r) from the memory\n",
        "      batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = replay_buffer.sample(batch_size)\n",
        "      state = torch.Tensor(batch_states).to(device)\n",
        "      next_state = torch.Tensor(batch_next_states).to(device)\n",
        "      action = torch.Tensor(batch_actions).to(device)\n",
        "      reward = torch.Tensor(batch_rewards).to(device)\n",
        "      done = torch.Tensor(batch_dones).to(device)\n",
        "      \n",
        "      # Step 5: From the next state s’, the Actor target plays the next action a’\n",
        "      next_action = self.actor_target(next_state)\n",
        "      \n",
        "      # Step 6: We add Gaussian noise to this next action a’ and we clamp it in a range of values supported by the environment\n",
        "      noise = torch.Tensor(batch_actions).data.normal_(0, policy_noise).to(device)\n",
        "      noise = noise.clamp(-noise_clip, noise_clip)\n",
        "      next_action = (next_action + noise).clamp(-self.max_action, self.max_action)\n",
        "      \n",
        "      # Step 7: The two Critic targets take each the couple (s’, a’) as input and return two Q-values Qt1(s’,a’) and Qt2(s’,a’) as outputs\n",
        "      target_Q1, target_Q2 = self.critic_target(next_state, next_action)\n",
        "      \n",
        "      # Step 8: We keep the minimum of these two Q-values: min(Qt1, Qt2)\n",
        "      target_Q = torch.min(target_Q1, target_Q2)\n",
        "      \n",
        "      # Step 9: We get the final target of the two Critic models, which is: Qt = r + γ * min(Qt1, Qt2), where γ is the discount factor\n",
        "      target_Q = reward + ((1 - done) * discount * target_Q).detach()\n",
        "      \n",
        "      # Step 10: The two Critic models take each the couple (s, a) as input and return two Q-values Q1(s,a) and Q2(s,a) as outputs\n",
        "      current_Q1, current_Q2 = self.critic(state, action)\n",
        "      \n",
        "      # Step 11: We compute the loss coming from the two Critic models: Critic Loss = MSE_Loss(Q1(s,a), Qt) + MSE_Loss(Q2(s,a), Qt)\n",
        "      critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
        "      \n",
        "      # Step 12: We backpropagate this Critic loss and update the parameters of the two Critic models with a SGD optimizer\n",
        "      self.critic_optimizer.zero_grad()\n",
        "      critic_loss.backward()\n",
        "      self.critic_optimizer.step()\n",
        "      \n",
        "      # Step 13: Once every two iterations, we update our Actor model by performing gradient ascent on the output of the first Critic model\n",
        "      if it % policy_freq == 0:\n",
        "        actor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n",
        "        self.actor_optimizer.zero_grad()\n",
        "        actor_loss.backward()\n",
        "        self.actor_optimizer.step()\n",
        "        \n",
        "        # Step 14: Still once every two iterations, we update the weights of the Actor target by polyak averaging\n",
        "        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
        "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "        \n",
        "        # Step 15: Still once every two iterations, we update the weights of the Critic target by polyak averaging\n",
        "        for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
        "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "  \n",
        "  # Making a save method to save a trained model\n",
        "  def save(self, filename, directory):\n",
        "    torch.save(self.actor.state_dict(), '%s/%s_actor.pth' % (directory, filename))\n",
        "    torch.save(self.critic.state_dict(), '%s/%s_critic.pth' % (directory, filename))\n",
        "  \n",
        "  # Making a load method to load a pre-trained model\n",
        "  def load(self, filename, directory):\n",
        "    self.actor.load_state_dict(torch.load('%s/%s_actor.pth' % (directory, filename)))\n",
        "    self.critic.load_state_dict(torch.load('%s/%s_critic.pth' % (directory, filename)))\n",
        "\n",
        "def evaluate_policy(policy, eval_episodes=10):\n",
        "  avg_reward = 0.\n",
        "  for _ in range(eval_episodes):\n",
        "    obs = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "      action = policy.select_action(np.array(obs))\n",
        "      obs, reward, done, _ = env.step(action)\n",
        "      avg_reward += reward\n",
        "  avg_reward /= eval_episodes\n",
        "  print (\"---------------------------------------\")\n",
        "  print (\"Average Reward over the Evaluation Step: %f\" % (avg_reward))\n",
        "  print (\"---------------------------------------\")\n",
        "  return avg_reward\n",
        "\n",
        "env_name = \"AntBulletEnv-v0\"\n",
        "seed = 0\n",
        "\n",
        "file_name = \"%s_%s_%s\" % (\"TD3\", env_name, str(seed))\n",
        "print (\"---------------------------------------\")\n",
        "print (\"Settings: %s\" % (file_name))\n",
        "print (\"---------------------------------------\")\n",
        "\n",
        "eval_episodes = 10\n",
        "save_env_vid = True\n",
        "env = gym.make(env_name)\n",
        "max_episode_steps = env._max_episode_steps\n",
        "if save_env_vid:\n",
        "  env = wrappers.Monitor(env, monitor_dir, force = True)\n",
        "  env.reset()\n",
        "env.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.shape[0]\n",
        "max_action = float(env.action_space.high[0])\n",
        "policy = TD3(state_dim, action_dim, max_action)\n",
        "policy.load(file_name, './pytorch_models/')\n",
        "_ = evaluate_policy(policy, eval_episodes=eval_episodes)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------------------------------\n",
            "Settings: TD3_AntBulletEnv-v0_0\n",
            "---------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1830.932278\n",
            "---------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}